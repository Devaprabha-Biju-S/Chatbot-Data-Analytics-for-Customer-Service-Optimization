# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rL4nbUs1jnTaz2v49EWotz8oe2Z-A_aJ
"""

# Colab cell: install Java, Spark, Python libraries
# Run this cell (may take ~2-4 minutes)
!apt-get update -qq
!apt-get install -y openjdk-11-headless -qq

# Choose a Spark version that pairs with Hadoop build (local mode no HDFS required)
SPARK_VERSION="3.3.2"
HADOOP_VERSION="3"
!wget -q "https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz"
!tar -xzf "spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz" -C /usr/local/
!ln -s /usr/local/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION /usr/local/spark

# Python packages
!pip install -q pyspark==3.3.2 findspark kaggle pandas matplotlib nltk vaderSentiment

# (optional) for nicer tables/plots
!pip install -q plotly

# Colab cell: upload kaggle.json
from google.colab import files
uploaded = files.upload()  # choose kaggle.json from your local machine

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Replace dataset slug if different. This is the example you gave.
!kaggle datasets download -d thoughtvector/customer-support-on-twitter -q

# unzip
!unzip -q customer-support-on-twitter.zip -d /content/kaggle_data
!ls -lah /content/kaggle_data

!apt-get install openjdk-11-jdk -y
!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
!tar xf spark-3.5.0-bin-hadoop3.tgz
!mv spark-3.5.0-bin-hadoop3 /usr/local/spark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/usr/local/spark"

!rm -rf /usr/local/spark
!apt-get install openjdk-11-jdk -y

!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
!tar xf spark-3.5.0-bin-hadoop3.tgz
!mv spark-3.5.0-bin-hadoop3 /usr/local/spark

!rm -rf /usr/local/spark
!apt-get install openjdk-11-jdk -y

!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
!ls -lh spark-3.5.0-bin-hadoop3.tgz

!tar xf spark-3.5.0-bin-hadoop3.tgz
!mv spark-3.5.0-bin-hadoop3 /usr/local/spark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/usr/local/spark"
os.environ["PATH"] += ":/usr/local/spark/bin:/usr/local/spark/sbin"

!pip install pyspark==3.5.0 findspark
import findspark
findspark.init()

import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Customer Support on Twitter Analysis") \
    .getOrCreate()

print("✅ Spark version:", spark.version)

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d thoughtvector/customer-support-on-twitter

!unzip customer-support-on-twitter.zip -d customer_support_data
!ls customer_support_data

!ls

!unzip -o customer-support-on-twitter.zip -d customer_support_data
!ls customer_support_data

df = spark.read.csv("/content/customer_support_data/twcs/twcs.csv", header=True, inferSchema=True)

df.printSchema()
df.show(5)

# Number of rows and columns
print("Total rows:", df.count())
print("Total columns:", len(df.columns))

# Show column names
print("\nColumns:", df.columns)

# View some example tweets
df.select("author_id", "inbound", "text").show(5, truncate=80)

from pyspark.sql.functions import col, sum as _sum

df.select([_sum(col(c).isNull().cast("int")).alias(c) for c in df.columns]).show()

df.groupBy("author_id").count().orderBy("count", ascending=False).show(10)

sample_df = df.limit(5000).toPandas()
sample_df.head()

!pip install textblob
from textblob import TextBlob
import pandas as pd

def get_sentiment(text):
    if pd.isna(text):
        return 0
    return TextBlob(text).sentiment.polarity

sample_df["sentiment"] = sample_df["text"].apply(get_sentiment)
sample_df[["text","sentiment"]].head()

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.hist(sample_df["sentiment"], bins=30, color="skyblue", edgecolor="black")
plt.title("Sentiment Distribution of Customer Tweets")
plt.xlabel("Sentiment (−1 = Negative, +1 = Positive)")
plt.ylabel("Tweet Count")
plt.show()

sentiment_spark = spark.createDataFrame(sample_df)

from pyspark.sql.functions import avg, col

sentiment_spark.groupBy("in_response_to_screen_name") \
    .agg(avg("sentiment").alias("avg_sentiment")) \
    .orderBy(col("avg_sentiment").asc()) \
    .show(10)

print(df.columns)

from pyspark.sql.functions import avg, col

sentiment_spark.filter(col("inbound") == False) \
    .groupBy("author_id") \
    .agg(avg("sentiment").alias("avg_sentiment")) \
    .orderBy(col("avg_sentiment").asc()) \
    .show(10)

from pyspark.sql.functions import col

# Give each dataframe a short alias
customer_msgs = df.filter(col("inbound") == True).alias("cust")
company_replies = df.filter(col("inbound") == False).alias("comp")

# Perform the join using aliases to avoid ambiguity
conversation_pairs = customer_msgs.join(
    company_replies,
    col("cust.tweet_id") == col("comp.in_response_to_tweet_id"),
    "inner"
).select(
    col("cust.author_id").alias("customer_id"),
    col("comp.author_id").alias("company_id"),
    col("cust.text").alias("customer_text"),
    col("comp.text").alias("company_reply"),
    col("comp.created_at").alias("reply_time")
)

conversation_pairs.show(5, truncate=80)

!pip install sentence-transformers
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('all-MiniLM-L6-v2')

def compute_similarity(text1, text2):
    emb1 = model.encode(text1, convert_to_tensor=True)
    emb2 = model.encode(text2, convert_to_tensor=True)
    return util.pytorch_cos_sim(emb1, emb2).item()

# Apply on a few pairs
conversation_pairs.limit(5).toPandas().apply(
    lambda row: compute_similarity(row['customer_text'], row['company_reply']), axis=1
)

from textblob import TextBlob

def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

# Example: apply to customer and reply
pair = conversation_pairs.limit(10).toPandas()
pair['customer_sentiment'] = pair['customer_text'].apply(get_sentiment)
pair['reply_sentiment'] = pair['company_reply'].apply(get_sentiment)
pair['sentiment_gap'] = pair['customer_sentiment'] - pair['reply_sentiment']
pair.head()

pairs_pd = conversation_pairs.limit(500).toPandas()  # limit to 500–1000 for faster testing
pairs_pd.head()

# 1️⃣ Semantic similarity
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('all-MiniLM-L6-v2')

def sim(a,b):
    return util.cos_sim(model.encode(a), model.encode(b)).item()

pairs_pd['similarity'] = pairs_pd.apply(lambda r: sim(r.customer_text, r.company_reply), axis=1)

# 2️⃣ Sentiment scores
from textblob import TextBlob
pairs_pd['cust_sent']  = pairs_pd['customer_text'].apply(lambda t: TextBlob(t).sentiment.polarity)
pairs_pd['reply_sent'] = pairs_pd['company_reply'].apply(lambda t: TextBlob(t).sentiment.polarity)
pairs_pd['sent_gap']   = pairs_pd['cust_sent'] - pairs_pd['reply_sent']

pairs_pd['is_bad'] = ((pairs_pd['similarity'] < 0.45) &
                     ((pairs_pd['cust_sent'] < -0.2) | (pairs_pd['sent_gap'] > 0.3)))
pairs_pd.head()

pairs_pd.columns

summary = pairs_pd.groupby('company_id').agg(
    total_pairs=('similarity', 'count'),
    pct_bad=('is_bad', 'mean'),
    avg_sim=('similarity', 'mean'),
    avg_sentgap=('sent_gap', 'mean')
).sort_values('pct_bad', ascending=False)

summary.head(10)

import matplotlib.pyplot as plt

summary.head(10).plot(
    y='pct_bad', kind='bar', figsize=(10,5),
    legend=False, color='tomato'
)
plt.title('Top 10 Companies with Most Response Accuracy Gaps')
plt.ylabel('% of Bad Responses')
plt.xlabel('Company ID')
plt.show()